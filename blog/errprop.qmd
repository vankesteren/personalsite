---
title: "Propagating uncertainty through nonparametric measurement modelling and posterior simulation"
author: "Erik-Jan van Kesteren"
format: 
  html: 
    standalone: true
    self-contained: true
---

```{r}
#| label: setup
#| include: false
#| message: false

library(tidyverse)
library(mgcv)
library(gratia)
library(mice)
```

## Introduction

Measurement error is becoming more common as automatic data labelling is occurring at larger scales. For example, large language models (LLMs) are now routinely used to turn unstructured data into structured tables, to measure particular constructs of interest. Our goal with these measurements, at least in the social sciences, is to investigate a certain real-world process or test a theory, generally by performing inference using a statistical model (such as linear regression). In this post, I show how to propagate measurement error resulting from the labeling process to such a statistical model -- without having to do tedious derivations specific to a particular model type ðŸ™‚

Conceptually, assume a true value $X_{true}$ can be measured only with error, yielding $X_{obs}$. This error can have two types components: a fixed (deterministic) component and an independent random (stochastic) component. For our purposes, the fixed error can be an offset (intercept) or it could be any function that is invertible over the domain of the data. 

```{r}
#| label: fixrand
#| echo: false
#| include: true
#| fig-width: 8
#| fig-height: 3
#| fig-align: center
#| message: false

set.seed(23)
N <- 230
x_true <- rnorm(N)
err_rand <- rnorm(N, sd = 0.3)
f <- \(x) 0.8*x - .15*x^2 + .1
x_rand <- x_true + err_rand
x_fix <- f(x_true)
x_both <- f(x_fix + err_rand)

df <- tibble(
  true = rep(x_true, 4), 
  obs = c(x_true, x_rand, x_fix, x_both), 
  label = as_factor(rep(c("No error", "Random error", "Fixed error", "Both errors"), each = N))
)

ggplot(df, aes(x = obs, y = true, colour = label)) +
  facet_grid(cols = vars(label)) +
  coord_fixed() +
  geom_point() +
  geom_abline(slope = 1, linetype = "dashed", alpha = .5) +
  labs(x = "Measured value", y = "True value", title = "Different types of measurement error") +
  scale_colour_brewer(palette = "Set2", guide = "none") +
  theme_light()

```

To fix this, we are going to learn the deterministic and stochastic error from calibration data using a nonparametric model, in our case regularized smoothing splines through the `R` package `mgcv`. Then, we will use this model to generate posterior samples from newly obtained measurements (with error). Last, we fit the model of interest several times and pool the results using an old trick from the multiple imputation literature.


# Step 1: Calibration model

Assume we want to perform linear regression with a single predictor $X$ and a single outcome $Y$. After collecting calibration data, we notice that both are measured with error:

```{r}
#| label: xy error
#| include: true
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| fig-align: center
N_cal <- 100

b <- 0.45
a <- -0.2

f_obs_x <- function(x) x + 0.6*sin(1.5*x)
sd_obs_x <- 0.25
f_obs_y <- function(y) 0.91*y + 1
sd_obs_y <- 0.2

x <- rnorm(N_cal)
y <- a + b*x + rnorm(N_cal, sd = 0.2) 

x_obs <- f_obs_x(x) + rnorm(N_cal, sd = sd_obs_x)
y_obs <- f_obs_y(y) + rnorm(N_cal, sd = sd_obs_y)

pxy <- tibble(
  true = c(x, y), 
  observed = c(x_obs, y_obs), 
  variable = as_factor(rep(c("X", "Y"), each = N_cal))
) |> 
  ggplot(aes(x = observed, y = true, colour = variable)) +
  geom_point() +
  geom_abline(slope = 1, linetype = "dashed", alpha = .5) +
  coord_fixed() +
  facet_grid(cols = vars(variable)) +
  labs(x = "Measured value", y = "True value", title = "X and Y are measured with error") +
  scale_colour_brewer(palette = "Set2", guide = "none") +
  theme_light()

pxy

```


Now, we create a model for each, using our penalized smooth estimator. You can use anything you want, as long as it can do reasonable posterior simulation with good (conditional) coverage. In other words, this should be a model that can (a) capture different types of relations, including nonlinear, (b) predict well (on average) in out-of-sample situations, and (c) accurately represent the residual noise. Penalized splines do all of this for a wide class of data-generating processes.

```{r}
#| label: model
#| echo: true

mod_x <- gam(x ~ s(x_obs))
mod_y <- gam(y ~ s(y_obs))

```

```{r}
#| label: plotmodel
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| fig-align: center

# visualising the calibration model
pred_df_y <- tibble(.row = 1:length(seq(-3, 3, .1)), y_obs = seq(-3, 3, .1))
df_ypred <- 
  posterior_samples(mod_y, data = pred_df_y, n = 10000) |> 
  group_by(.row) |> 
  summarize(true = mean(.response), lo = quantile(.response, 0.055), hi = quantile(.response, 0.945)) |>
  left_join(pred_df_y, by = join_by(.row)) |> 
  rename(observed = y_obs)

pred_df_x <- tibble(.row = 1:length(seq(-3, 3, .1)), x_obs = seq(-3, 3, .1))
df_xpred <- 
  posterior_samples(mod_x, data = pred_df_x, n = 10000) |> 
  group_by(.row) |> 
  summarize(true = mean(.response), lo = quantile(.response, 0.055), hi = quantile(.response, 0.945)) |>
  left_join(pred_df_x, by = join_by(.row)) |> 
  rename(observed = x_obs)

df_pred <- bind_rows(df_xpred |> mutate(variable = "X"), df_ypred |> mutate(variable = "Y"))


pxy +
  geom_ribbon(aes(ymin = lo, ymax = hi), data = df_pred, fill = "seagreen", alpha = 0.1, color = NA) +
  geom_line(data = df_pred, color = "seagreen") +
  labs(title = "Penalized spline measurement error models")

```

# Step 2: generate posterior samples

After the calibration data, we collect our "real" data, where we don't have gold-standard "true" measurements available. We now have a standard missing data situation. What we do, then, is to predict those values from the observed data, and perform posterior sampling to adequately account for the uncertainty. For example, we can sample 200 times. Then, we simply run the model of interest on each dataset, and collect the results in a list:

```{r}
#| label: realdata
#| echo: false

# generate real data
set.seed(45)
N <- 79
x_true <- rnorm(N)
y_true <- a + b*x_true + rnorm(N, sd = 0.2) 

x_obs <- f_obs_x(x_true) + rnorm(N, sd = sd_obs_x)
y_obs <- f_obs_y(y_true) + rnorm(N, sd = sd_obs_y)
```



```{r}
#| label: sample
#| echo: true

sim_dat <- lapply(1:200, function(i) {
  data.frame(
    x = simulate(mod_x, data = data.frame(x_obs = x_obs))[,1],
    y = simulate(mod_y, data = data.frame(y_obs = y_obs))[,1]
  )
})

sim_fit <- lapply(sim_dat, function(df) lm(y ~ x, data = df))

```


We have now produced a set of models based on corrected datasets. To get an intuition for the correction, below is a plot with the true (unobserved, hidden) data, the measured (observed, with error) data, and one posterior sample from the model-corrected data.

```{r}
#| label: sampleplot
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| fig-align: center

tibble(
  x = c(x_true, x_obs, sim_dat[[1]]$x), 
  y = c(y_true, y_obs, sim_dat[[1]]$y),
  type = as_factor(rep(c("True (unobserved)", "Measured", "Corrected (posterior sample)"), each = N))
) |> 
  ggplot(aes(x = x, y = y, color = type)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y~x) +
  facet_grid(cols = vars(type)) +
  labs(x = "X", y = "Y", title = "True, measured, and corrected data") +
  scale_colour_brewer(palette = "Set2", guide = "none") +
  theme_light()

```

In addition, I've plotted the regression lines. As shown, the third regression line looks a lot like the first, whereas the regression fitted on the measured model shows a quite different intercept and a flatter slope.

# Step 3: Pooling multiple model fits

To perform adequate inference, we need to combine the model fits we created by sampling from the posterior of the calibration model. There is a lot of existing research on this topic already, with most coming from the missing data literature on multiple imputation methods. Specifically, we can apply Rubin's pooling rules to combine the parameter estimates and their uncertainties to arrive at an overall inference for the parameters in the model.

First, let's look at a model based on the unobserved "true" data, our target is to be as close as possible to these estimates:

```{r}
#| label: modtrue
#| echo: true

tidy(lm(y_true ~ x_true))
```

Then, let's look at the model we would get if we would naÃ¯vely apply regression on the uncorrected observed data:

```{r}
#| label: modobs
#| echo: true

tidy(lm(y_obs ~ x_obs))
```

Finally, let's pool the results from our posterior-sampled data.

```{r}
#| label: modpool
#| echo: true

tibble(summary(pool(sim_fit)))
```

The naive model deviates strongly from the true model, whereas the pooled model fares much better in terms of the parameter estimates. The standard errors of the pooled model are bigger, which reflects the uncertainty stemming from the stochastic measurement error as captured in the nonparametric calibration model. Pooling can be done in the `mice` package for many different inferential models, from simple linear regression to complex mixed model types.

# Conclusion

By learning a calibration model on data with stochastic and/or deterministic measurement error, we can correct inferences in a downstream analysis task through posterior sampling. A model pooling trick from the multiple imputation / missing data literature is used to combine sampled models. This generalizes to any type of variable and any type of model, assuming that the estimated posterior distribution is correct.
